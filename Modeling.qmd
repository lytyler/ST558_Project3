---
title: "Modeling: Diabetes Health Indicators"
author: "Lanette Tyler"
format: html
editor: visual
editor_options: 
  chunk_output_type: console
---

## Introduction

### Data: Diabetes Binary Health Indicators

The data used in this analysis is from the Diabetes Health Indicators Data Set available at kaggle. The specific data file used is diabetes_binary_health_indicators_BRFSS2015.csv. This file consists of a cleaned data set of 253,680 survey responses to the CDC's Behavioral Risk Factor Surveillance System (RFSS) 2015. This analysis is focused on the lifestyle factors of smoking, drinking, physical activity, eating fruits and vegetables, as well as the basic body features of BMI, sex, and age, with the goal of building a model of these variables to predict diabetes status. A more detailed description of the variables is given in the [EDA portion](EDA.html) of the project.

[Learn more about the original data here.](https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset/data?select=diabetes_binary_health_indicators_BRFSS2015.csv)

### Purpose

The purpose of this modeling exercise is to fit both classification tree and random forest models to the variables under consideration in order to predict diabetes status. A best fit model of each type will be selected, and then an overall best fit will be selected. The modeling data is split into training and test sets, five-fold cross-validation is used to to fit each model type, and log-loss is used as the metric for selecting models. The best fit for each model type will be fit on the entire training data set, and then an overall best model will be chosen. THe final model will be fit again on the entire data set.

The ultimate goal of the entire project is to fit predictive models to the variables, choose a best fit model of each type and then an overall best fit, and then house that model in an API within a docker container tk make model predictions accessible to others.

## Preliminary Tasks

*Load packages, read in and transform data, and create data set for modeling.*

Load packages:

```{r}
library(tidyverse)
library(tidymodels)
```

Read in and transform data:
*Applying same transformations as in EDA.qmd, except not adding labels this time*

```{r}
original_diabetes_data <- read_csv("diabetes_binary_health_indicators_BRFSS2015.csv")

d_data <- original_diabetes_data |>
  rename("dia_ind" = "Diabetes_binary",
         "bp" = "HighBP",
         "chol" = "HighChol",
         "chol_ch5" = "CholCheck",
         "bmi" = "BMI",
         "smoker" = "Smoker",
         "stroke" = "Stroke",
         "hrt_dis_atk" = "HeartDiseaseorAttack",
         "phys_act" = "PhysActivity",
         "fruits" = "Fruits",
         "veggies" = "Veggies",
         "alcohol" = "HvyAlcoholConsump",
         "h_care" = "AnyHealthcare",
         "no_doc" = "NoDocbcCost",
         "gen_health" = "GenHlth", 
         "m_health" ="MentHlth", 
         "ph_health" = "PhysHlth",
         "walk_diff" = "DiffWalk", 
         "sex" = "Sex", 
         "age" ="Age", 
         "edu"  ="Education", 
         "income" = "Income")

d_data <- d_data |> mutate(
  dia_ind = factor(dia_ind, levels = c(0, 1), 
                   labels = c("No Diabetes", "Diabetes")), 
  bp = factor(bp, levels = c(0, 1), 
              labels = c("No High BP", "High BP")), 
  chol = factor(chol, levels = c(0, 1), 
                labels = c("No High Cholesterol", "High Cholesterol")), 
  chol_ch5 = factor(chol_ch5, levels = c(0, 1),
                    labels = c("No 5yr Cholesterol Check", "5yr Cholesterol Check")),
  smoker = factor(smoker, levels = c(0, 1),
                  labels = c("Non-Smoker", "Smoker")),
  stroke = factor(stroke, levels = c(0, 1),
                  labels = c("No Stroke", "Stroke")),      
  hrt_dis_atk = factor(hrt_dis_atk, levels = c(0, 1),
                  labels = c("No Heart Disease/Attack", "Heart Disease/Attack")),
  phys_act = factor(phys_act, levels = c(0, 1),
                  labels = c("No Physical Activity", "Physical Activity")), 
  fruits = factor(fruits, levels = c(0, 1),
                  labels = c("No Fruit", "Fruit")), 
  veggies = factor(veggies, levels = c(0, 1),
                  labels = c("No Veggies", "Veggies")),     
  alcohol = factor(alcohol, levels = c(0, 1),
                  labels = c("Not Heavy Drinker", "Heavy Drinker")),    
  h_care = factor(h_care, levels = c(0, 1),
                  labels = c("No Healthcare Coverage", "Healthcare Coverage")),      
  no_doc = factor(no_doc, levels = c(0, 1),
                  labels = c("No Missed Visit Due to Money", "Missed Visit Due to Money")),      
  gen_health = factor(gen_health, levels = c(1, 2, 3, 4, 5),
                      labels = c("Excellent General", "Very Good General Health", 
                                 "Good General Health", "Fair General Health", "Poor General Health")),
  m_health = factor(m_health, levels = c(0:30),
                      labels = c(0:30)),
  ph_health = factor(ph_health, levels = c(0:30),
                      labels = c(0:30)),
  walk_diff = factor(walk_diff, levels = c(0, 1),
                      labels = c("No Difficulty Walking", "Difficulty Walking")),
  sex = factor(sex, levels = c(0, 1),
                  labels = c("Female", "Male")),
  age = factor(age, levels = c(1:13),
               labels = c("Ages 18 to 24", 
                          "Ages 25 to 29", 
                          "Ages 30 to 34", 
                          "Ages 35 to 39",
                          "Ages 40 to 44", 
                          "Ages 45 to 49", 
                          "Ages 50 to 54", 
                          "Ages 55 to 59",
                          "Ages 60 to 64",
                          "Ages 65 to 69",
                          "Ages 70 to 74",
                          "Ages 75 to 79",
                          "Ages 80 or older")),         
  edu = factor(edu, levels = c(1:6),
               labels = c("Kindergarten or Less", "Grades 1 to 8", "Some High School", "High School Graduate", "Some College or Technical School", "College Graduate")),
  income = factor(income, levels = c(1:8),
                  labels = c("Less than $10,000",
                             "$10,000 to less than $15,000",
                             "$15,000 to less than $20,000",
                             "$20,000 to less than $25,000",
                             "$25,000 to less than $35,000", 
                             "$35,000 to less than $50,000",
                             "$50,000 to less than $75,000",
                             "$75,000 or more")))

```


Create modeling data file:

```{r}
m_data <- d_data |> select(dia_ind, bmi, smoker, phys_act, fruits, veggies, alcohol, sex, age)
```

## Split Data

*Split data into training and test sets, create folds for cross-validation (CV).*

Split data into training and test sets:

```{r}
set.seed(50)
m_split <- initial_split(m_data, prop = 0.7, strata = dia_ind)
m_train <- training(m_split)
m_test <- testing(m_split)
m_train #take a look
m_test #take a look
```

Create CV folds on training data:

```{r}
m_CV_folds <- vfold_cv(m_train, 5)
```

## Classification Tree Model

A **classification tree** is a type of **decision tree**. It is a flexible, nonlinear model in which the predictor space is divided into regions, and a prediction value is assigned to each region. The most common classification for a region is usually assigned to that region, which is what is done here. The modeling algorithm determines the best variable to start with (the most influential) and where to split the variables. The values of the variables determine the value of the prediction, according to the fitted decision tree.

Using the tidymodels framework, we will create a recipe, a model engine, and a workflow; fit the workflow to the CV folds; collect metrics and determine best classification tree model (best tuning parameter(s)), finalize the workflow and fit that best model to the entire training data set.

Create recipe:

```{r}
cl_tree_recipe <- recipe(dia_ind ~ ., data = m_train) |>
  step_log(bmi) |>
  step_dummy(smoker, phys_act, fruits, veggies, alcohol, sex, age) |>
  step_normalize(all_numeric())
```

Create model engine:

```{r}
cl_tree_model <- decision_tree(tree_depth = 6,
                          min_n = 20,
                          cost_complexity = tune()) |>
  set_engine("rpart") |>
  set_mode("classification")
```

Create workflow:

```{r}
cl_tree_wkflow <- workflow() |>
  add_recipe(cl_tree_recipe) |>
  add_model(cl_tree_model)
```

Create fits:

```{r}
#first create grid
cl_tree_grid <- grid_regular(cost_complexity(),
                          levels = 15)

#then create fits with grid
cl_tree_fits <- cl_tree_wkflow |> 
  tune_grid(resamples = m_CV_folds,
            grid = cl_tree_grid,
            metrics = metric_set(mn_log_loss))
```

Collect metrics:

```{r}
cl_tree_fits |>
  collect_metrics() |>
  arrange(mean)
```

Choose the best fit classification tree model/best cost_complexity parameter:

```{r}
cl_tree_best_parameters <- select_best(cl_tree_fits, metric = "mn_log_loss")
cl_tree_best_parameters #take a look
```

Finalize workflow and fit model to the entire training set:

```{r}
cl_tree_best_fit <- cl_tree_wkflow |>
  finalize_workflow(cl_tree_best_parameters) |>
  last_fit(m_split, metrics = metric_set(mn_log_loss))
cl_tree_best_fit #take a look
```

We now have the fit, plus the metrics for how this fit performs on the test set.

```{r}
cl_tree_best_fit |>
  collect_metrics()
```

View plot of the model:

```{r}
cl_tree_best_fit |>
  extract_fit_engine() |>
  rpart.plot::rpart.plot(roundint = FALSE)
```

The classification tree splits first on BMI.

## Random Forest Model

The regression tree model just fit to the diabetes data was an individual tree model. When prediction is the only goal, ensemble tree models can be used. **Ensemble tree models** are built from an average of many trees. To build many trees, bootstrapping is used to create many data samples. Each sample is then used to build an individual tree model, and the ensemble model is an average of the many individual models. This method decreases variance over individual tree fits.

Non-parametric bootstrapping involves treating the original data sample as the population. Additional samples are created by drawing from the "population" with replacement. Building ensemble tree models using bootstrap aggregation is called **bagging**. The prediction is the most common classification for that region of predictor space across all of the fitted trees.

**Random forest models** are based on the same ideas as bagging, except that a random, set number of predictors is used at each step instead of using all of the predictors for each split. This improves the model over bagging in situations where a very strong predictor exists, causing more correlation between trees resulting in a smaller reduction in variance from aggregation. Random forest models protect against domination of the model by one or two good predictors.

Similar to the classification tree model, a random forest model will be generated for the diabetes data set using the tidymodels framework by defining a recipe, a model specification, and a workflow; fitting the workflow to the CV folds, collecting metrics and determining the best random forest model (best tuning parameter(s)), and fitting that best model to the entire training data set.

Create recipe:

```{r}
random_forest_recipe <- recipe(dia_ind ~ ., data = m_train) |>
  update_role(age, new_role = "ID") |>
  step_log(bmi) |>
  step_dummy(smoker, phys_act, fruits, veggies, alcohol, sex) |>
  step_normalize(all_numeric())
```

Create model engine:

```{r}
random_forest_model <- rand_forest(mtry = tune(), trees = 50) |>
  set_engine("ranger", importance = "impurity") |>
  set_mode("classification")
```

The model is tuning for the number of predictors to be randomly sampled at each split (mtry).

Create workflow:

```{r}
random_forest_wkflow <- workflow() |>
  add_recipe(random_forest_recipe) |>
  add_model(random_forest_model)
```

Fit workflow to CV folds:

```{r}
random_forest_fits <- random_forest_wkflow |>
  tune_grid(resamples = m_CV_folds, 
            grid = 7,
            metrics = metric_set(mn_log_loss))
```

Collect metrics:

```{r}
random_forest_fits |>
  collect_metrics() |>
  arrange(mean)
```

Determine best random forest model fit/best mtry parameter:

```{r}
random_forest_best_parameters <- select_best(random_forest_fits,
                                             metric = "mn_log_loss")
random_forest_best_parameters #take a look
```

Finalize workflow and fit model to the entire training set:

```{r}
random_forest_best_fit <- random_forest_wkflow |>
  finalize_workflow(random_forest_best_parameters) |>
  last_fit(m_split, metrics = metric_set(mn_log_loss))
random_forest_best_fit #take a look
```

We now have the fit, plus the metrics for how this fit performs on the test set.

```{r}
random_forest_best_fit |>
  collect_metrics()
```

Visualize the random forest best fit with a variable importance plot:

```{r}
#function for extracting importance values
get_rf_imp <- function(random_forest_object) {
    extract_fit_parsnip(random_forest_object) |>
    vip::vi()
}

# use the function on the best random forest fit
rf_importance <- get_rf_imp(random_forest_best_fit)

#plot the importance values
ggplot(rf_importance, aes(x = Importance, y = Variable)) +
  geom_bar(stat = "identity") +
  labs(title = "Random Forest Best Fit:", 
       subtitle = "Variable Importance Plot")
```

According to the plot, the most important variable considered is BMI, by far. This parallels the classification tree's first split on BMI. The next most important variable in the random forest according to the plot of the model is physical activity.

## Final Model Selection

Compare the performance of both best fit models on the test data:

```{r}
rbind(
cl_tree_best_fit |>
  collect_metrics() |>
  mutate("Model" = "Best Classification Tree") |>
  select(Model, everything()),
random_forest_best_fit |>
  collect_metrics() |>
  mutate("Model" = "Best Random Forest") |>
  select(Model, everything())
)
```

The random forest best model has lower mean log-loss than the classification tree best model. The best-fit random forest model is the final model selected.

## Final Model

Fit the overall best model (the best random forest model) to the entire data set.

```{r}
final_model <- random_forest_wkflow |>
  finalize_workflow(random_forest_best_parameters) |>
  fit(m_data)
```

View the final model as a variable importance plot:

```{r}
var_importance <- get_rf_imp(final_model) #function to extract importance values

#plot the importance values
ggplot(var_importance, aes(x = Importance, y = Variable)) +
  geom_bar(stat = "identity") +
  labs(title = "Variable Importance Plot: Final Model", 
       subtitle = "Best Random Forest Model Fit on Entire Data Set")
```

Export modeling data set and final model for use in associated API:

```{r}
write_csv(m_data, "modelAPI.R/diabetes_modeling_data.csv")
saveRDS(final_model, file = "modelAPI.R/final_model.rds")
```

